{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example for NCI users: Regional Tasmania JRA-55 and ACCESS-OM2\n",
    "\n",
    "**Before you begin, make sure you have access to the relevent projects to access the data listed below**\n",
    "\n",
    "## What does this notebook do?\n",
    "This notebook is designed to set you up with a working MOM6 regional configuration. First, try and get it running with our default Tasmania case, then you can clone the notebook and modify for your region of interest. \n",
    "\n",
    "Input Type | Source | Location on NCI\n",
    "---|---|---\n",
    "Surface | [JRA55 surface forcing](https://climatedataguide.ucar.edu/climate-data/jra-55) | `/g/data/ik11`\n",
    "Ocean | [ACCESS-OM2-01](https://data.marine.copernicus.eu/product/GLOBAL_MULTIYEAR_PHY_001_030/description) |  `/g/data/ik11`  \n",
    "Bathymetry | [GEBCO](https://www.gebco.net/data_and_products/gridded_bathymetry_data/) | `/g/data/ik11`\n",
    "\n",
    "Additionally, you'll need access to `/g/data/x77/` if you want to use the same executable using the latest FMS build (a good idea for troubleshooting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/home/149/ab8992/cosima_regional/development/regional-mom6/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## This example is somewhat NCI centric, but should be easily adaptable to other systems\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# To simply use Ashley's version of this package, uncomment the following:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/149/ab8992/cosima_regional/development/regional-mom6/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#IMPORTANT FOR NCI USERS: As of Nov 2023 you need to use analysis-unstable to get the latest version of xESMF or bathymetry regridding won't work\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/home/149/ab8992/cosima_regional/development/regional-mom6/'"
     ]
    }
   ],
   "source": [
    "## This example is somewhat NCI centric, but should be easily adaptable to other systems\n",
    "# To simply use Ashley's version of this package, uncomment the following:\n",
    "import os\n",
    "os.chdir(\"/home/149/ab8992/cosima_regional/development/regional-mom6/\")\n",
    "#IMPORTANT FOR NCI USERS: As of Nov 2023 you need to use analysis-unstable to get the latest version of xESMF or bathymetry regridding won't work\n",
    "\n",
    "import os\n",
    "import xarray as xr\n",
    "import regional_mom6 as rm\n",
    "from pathlib import Path\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does this package do?\n",
    "\n",
    "Setting up a regional model in MOM6 is a pain. The goal of this package is that users should spend their debugging time fixing a model that's running and doing weird things, rather than puzzling over a model that won't even start.\n",
    "\n",
    "In running this notebook, you'll hopefully have a running MOM6 regional model. There will still be a lot of fiddling to do with the MOM_input file to make sure that the parameters are set up right for your domain, and you might want to manually edit some of the input files. BUT, this package should help you bypass most of the woes of regridding, encoding and understanding the arcane arts of the MOM6 boundary segment files. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does this notebook do?\n",
    "\n",
    "This notebook demonstrates how to set up a regional domain using the package. By the end you should have a running MOM6 experiment on the domain of your choice. To make a stable test case:\n",
    "\n",
    "* Avoid any regions with ice\n",
    "* Avoid regions near the north pole\n",
    "* Although the default configuration is meant to be RYF, I've not fixed up the calendar and encoding to run longer than a year just yet\n",
    "\n",
    "\n",
    "Input Type | Source\n",
    "---|---\n",
    "Surface | JRA \n",
    "Ocean | ACCESS OM2-01\n",
    "Bathymetry | Gebco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Your personal environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch = \"/scratch/v45/ab8992\"\n",
    "home = \"/home/149/ab8992\"\n",
    "\n",
    "scratch = \"/scratch/ul08/ab8992\"\n",
    "home = \"/g/data/ul08/newcal-test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Choose our domain, define workspace paths\n",
    "\n",
    "To make sure that things are working I'd recommend starting with the default example defined below. If this runs ok, then change to a domain of your choice and hopefully it runs ok too! There's some troubleshooting you can do if not (check readme / readthedocs)\n",
    "\n",
    "To find the lat/lon of the domain you want to test you can use <a href=\"https://data.marine.copernicus.eu/product/GLOBAL_MULTIYEAR_PHY_001_030/download\" > this GUI </a> and copy paste below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_name = \"newcal\"\n",
    "\n",
    "## Choose your coordinates and the name of your experiment\n",
    "yextent = [-35,-0] ## latitude\n",
    "xextent = [160,190] ## longitude\n",
    "\n",
    "daterange = [\"1990-01-01 00:00:00\", \"1990-01-05 00:00:00\"] ## 2003 is a good compromise for GLORYs and JRA forcing as they overlap. JRA ends in 2012, GLORYS starts in 1993\n",
    "\n",
    "## Place where all your input files go\n",
    "inputdir = f\"{scratch}/regional_mom6_configs/{expt_name}/\"\n",
    "\n",
    "## Directory where you'll run the experiment from\n",
    "rundir = f\"{home}/mom6_rundirs/{expt_name}/\"\n",
    "\n",
    "## Directory where fre tools are stored\n",
    "toolpath = \"/home/157/ahg157/repos/mom5/src/tools/\" ## Compiled tools needed for construction of mask tables\n",
    "\n",
    "## Directory where ocean model cut-outs go before processing\n",
    "tmpdir = f\"{scratch}/regional_tmp/{expt_name}\"\n",
    "\n",
    "for i in [rundir,tmpdir,inputdir]:\n",
    "    if not os.path.exists(i):\n",
    "        os.makedirs(str(i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare ocean forcing data\n",
    "\n",
    "We need to cut out our ocean forcing. The pipeline expects an initial condition and one time-dependent segment per non-land boundary. Naming convention is \"east_unprocessed\" and \"ic_unprocessed\" for initial condition. The following provides an example for cutting out the necessary forcing files from an ocean model. It's hardcoded to pull data from a Repeat Year Forced ACCESS-OM2-01 database, but you should be able to recycle parts of the code to cut out data from a dataset of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**NOTE: this is hardcoded for the year of 1990, which corresponds to files 1077 - 1082. If you want to modify, you'll need to choose the right path to the year of your choice, or use the COSIMA cookbook to locate your data files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Cut out 3 months of forcing from 1990\n",
    "om2_input = xr.open_mfdataset(f\"/g/data/ik11/outputs/access-om2-01/01deg_jra55v13_ryf9091/output1077/ocean/ocean_daily*\",parallel=True,chunks='auto')[[\"u\",\"v\",\"salt\",\"temp\",\"eta_t\"]].sel(    \n",
    "    yu_ocean = slice(yextent[0] - 0.2,yextent[1] + 0.2),\n",
    "    yt_ocean = slice(yextent[0] - 0.2,yextent[1] + 0.2)\n",
    ").isel(time = slice(0,5))\n",
    "\n",
    "\n",
    "# Cut out initial condition and save\n",
    "ic = om2_input.isel(time = 0)\n",
    "\n",
    "## Nicer Slicer handles seams in longitude and different grids. Ensures that the output matches our 'xextent'\n",
    "ic = rm.nicer_slicer(ic,[xextent[0],xextent[1]],[\"xu_ocean\",\"xt_ocean\"])\n",
    "ic.to_netcdf(tmpdir + \"/ic_unprocessed\")\n",
    "\n",
    "## Cut out East and West segments. Does lat slice first then uses nicer slicer for lon slice\n",
    "eastwest = om2_input.sel(    \n",
    "    yu_ocean = slice(yextent[0] - 0.2,yextent[1] + 0.2),\n",
    "    yt_ocean = slice(yextent[0] - 0.2,yextent[1] + 0.2)\n",
    ")\n",
    "rm.nicer_slicer(eastwest,[xextent[1],xextent[1]],[\"xu_ocean\",\"xt_ocean\"]).to_netcdf(tmpdir + \"/east_unprocessed\")\n",
    "rm.nicer_slicer(eastwest,[xextent[0],xextent[0]],[\"xu_ocean\",\"xt_ocean\"]).to_netcdf(tmpdir + \"/west_unprocessed\")\n",
    "\n",
    "## Cut out North and South segments\n",
    "northsouth = rm.nicer_slicer(om2_input,[xextent[0],xextent[1]],[\"xu_ocean\",\"xt_ocean\"])\n",
    "northsouth.sel(\n",
    "    yu_ocean = slice(yextent[1] - 0.2,yextent[1] + 0.2),\n",
    "    yt_ocean = slice(yextent[1] - 0.2,yextent[1] + 0.2)\n",
    ").to_netcdf(tmpdir + \"/north_unprocessed\")\n",
    "northsouth.sel(\n",
    "    yu_ocean = slice(yextent[0] - 0.2,yextent[0] + 0.2),\n",
    "    yt_ocean = slice(yextent[0] - 0.2,yextent[0] + 0.2)\n",
    ").to_netcdf(tmpdir + \"/south_unprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Make experiment object\n",
    "This object keeps track of your domain basics, as well as generating the hgrid, vgrid and setting up the folder structures. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt = rm.experiment(\n",
    "    xextent,\n",
    "    yextent,\n",
    "    daterange,\n",
    "    0.05,  # Horizontal Resolution\n",
    "    75,    # Number of vertical layers\n",
    "    10,    # Ratio of largest to smallest vertical layer. Select 1 for linear, negative number for higher resolution at bottom\n",
    "    4500,  # Depth of simulation\n",
    "    rundir,\n",
    "    inputdir,\n",
    "    toolpath\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running you can have a look at your grids by calling `expt.hgrid` and `expt.vgrid`\n",
    "\n",
    "Plotting vgrid with marker = '.' option lets you see the spacing, or plotting \n",
    "```python\n",
    "np.diff(expt.vgrid.zl).plot(marker = '.')\n",
    "```\n",
    " shows you the vertical spacing profile.\n",
    "\n",
    " ### Modular workflow!\n",
    "\n",
    "After constructing your expt object, if you don't like the default hgrid and vgrids you can simply modify and overwrite them. However, you'll then also need to save them to disk again. For example:\n",
    "\n",
    "```python\n",
    "new_hgrid = xr.open_dataset(inputdir / \"hgrid.nc\")\n",
    "```\n",
    "Modify `new_hgrid`, ensuring that metadata is retained to keep MOM6 happy. Then, save your changes\n",
    "\n",
    "```python\n",
    "expt.hgrid = new_hgrid\n",
    "\n",
    "expt.hgrid.to_netcdf(inputdir / \"hgrid.nc\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set up bathymetry\n",
    "\n",
    "Similarly to ocean forcing, we point our 'bathymetry' method at the location of the file of choice, and pass it a dictionary mapping variable names. This time we don't need to preprocess the topography since it's just a 2D field and easier to deal with. Afterwards you can run `expt.topog` and have a look at your domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt.bathymetry(\n",
    "    '/g/data/ik11/inputs/GEBCO_2022/GEBCO_2022.nc',\n",
    "    {\"xh\":\"lon\",\n",
    "     \"yh\":\"lat\",\n",
    "     \"elevation\":\"elevation\"}, ## Again this dictionary just maps mom6 variable names to what they are in your topog.\n",
    "     minimum_layers = 1        ## Minimum number of layers allowed. Any areas with fewer layers are marked as land\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out your domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt.topog.depth.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 5: Handle the ocean forcing - where the magic happens\n",
    "\n",
    "This cuts out and interpolates the initial condition as well as all boundaries (unless you don't pass it boundaries).\n",
    "\n",
    "The dictionary maps the MOM6 variable names to what they're called in your ocean input file. Notice how the horizontal dimensions are x and y in the GLORYS reanalysis example, vs xh, yh, xq, yq. This is because ACCESS-OM2-01 is on a `B` grid, so we need to differentiate between `q` and `t` points. \n",
    "\n",
    "If one of your segments is land, you can delete its string from the 'boundaries' list. You'll need to update MOM_input to reflect this though so it knows how many segments to look for, and their orientations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt.ocean_forcing(\n",
    "    tmpdir,  ## Path to ocean foring files\n",
    "    {\"time\":\"time\",\n",
    "     \"yh\":\"yt_ocean\",\n",
    "     \"xh\":\"xt_ocean\",\n",
    "     \"xq\":\"xu_ocean\",\n",
    "     \"yq\":\"yu_ocean\",\n",
    "     \"zl\":\"st_ocean\",\n",
    "     \"eta\":\"eta_t\",\n",
    "     \"u\":\"u\",\n",
    "     \"v\":\"v\",\n",
    "     \"tracers\":{\"salt\":\"salt\",\"temp\":\"temp\"}},\n",
    "    boundaries = [\"south\",\"north\",\"west\",\"east\"],\n",
    "    gridtype=\"B\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 Run the FRE tools\n",
    "\n",
    "This is just a wrapper for the FRE tools needed to make the mosaics and masks for the experiment. The only thing you need to tell it is the processor layout. In this case we're saying that we want a 10 by 10 grid of 100 processors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt.FRE_tools((10,10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Modify the default input directory to make a (hopefully) runnable configuration out of the box\n",
    "\n",
    "This step copies the default directory, and modifies the `MOM_input` and `SIS_input` files to match your experiment. If you use Payu to run mom6, set the `using_payu` flag to `True` and an example `config.yaml` file will be copied to your run directory. This still needs to be modified manually to work with your projects, executable etc.\n",
    "\n",
    "If you've pip-installed the package, you'll need to know where the `regional-mom6` code was copied to. Alternatively, just clone the repo somewhere else and pass the path to the method below. This allows it to find and modify the default input directory included with the package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt.setup_run_directory(surface_forcing = \"jra\",using_payu = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Run your model!\n",
    "\n",
    "To do this, navigate to your run directory in terminal. If you're working on NCI, you can do this via:\n",
    "\n",
    "```\n",
    "module load conda/analysis3\n",
    "payu setup -f\n",
    "payu run -f\n",
    "```\n",
    "\n",
    "By default `input.nml` is set to only run for 5 days as a test. If this is successful, you can modify this file to then run for longer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 and beyond: Fiddling, troubleshooting and fine tuning\n",
    "\n",
    "Hopefully your model is running. If not, the first thing you should do is reduce the timestep. You can do this by adding `#override DT=XXXX` to your `MOM_override` file. \n",
    "\n",
    "If there's strange behaviour on your boundaries, you could play around with the `nudging timescale` (an example is already included in the `MOM_override` file). Sometimes, if your boundary has a lot going on (like all of the eddies spinning off the ACC), it can be hard to avoid these edge effects. This is because the chaotic, submesoscale structures developed within the regional domain won't match those at the boundary. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3-23.10]",
   "language": "python",
   "name": "conda-env-analysis3-23.10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
